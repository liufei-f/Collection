{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1925cec5-5b8c-450c-8b86-fc0ee19189f5",
   "metadata": {},
   "source": [
    "\n",
    "# 1. Zero-shot learning(零样本学习)\n",
    "模型能够在没有见过特定任务或样本标签的情况下，对新任务进行预测或分类。\n",
    "它依赖于模型对任务和数据的通用理解，通常通过以下方式实现：\n",
    "1. 利用先验知识：预训练\n",
    "2. 任务描述：通过自然语言描述任务（如文本提示）来指导模型预测，而无需提供特定的训练样本\n",
    "3. 共享特征空间：将未知类别的特征映射到已知类别共享的特征空间，基于语义或上下文进行推断\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71112bcd-9bff-4818-b8d9-0b62830e8387",
   "metadata": {},
   "source": [
    "# 2. Logical Reasoning(逻辑推理)\n",
    "模型根据已知事实/规则或模型进行逻辑分析和推导的能力。\n",
    "1. Deductive reasoning: 从一般规则推导出具体结论\n",
    "2. Inductive reasoning: 从具体例子总结出一般规律\n",
    "3. Abductive reasoning: 从结论推测最可能的原因\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb35f7-4cae-42cf-85b4-fd4f39aab32e",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Low-Rank Adaptation(LoRA)\n",
    "是一种高效适配预训练模型的方法，尤其在大规模语言模型或其他深度学习模型上表现出色。它通过引入低秩分解的思想，大幅减少模型参数调整的需求，从而降低计算开销和存储需求。\n",
    "\n",
    "在深度学习中，预训练的大模型通常含有大量的参数，直接对其进行fine-tuning需要巨大的计算资源和存储空间。LoRA通过引入低秩矩阵，将权重调整在一个低秩子空间中，极大的减少了参数调整的数量。\n",
    "1. 权重矩阵分解\n",
    "假设一个预训练模型的权重矩阵为$W$，LoRA假设权重的调$\\Delta W$是低秩的，可以分解为两个更小的矩阵：$$\\Delta W = A \\cdot B$$\n",
    "\n",
    "其中：\n",
    "-  $ A \\in \\mathbb{R}^{d \\times r} $\n",
    "-  $ B \\in \\mathbb{R}^{r \\times k} $\n",
    "-  $ r  是分解的秩（通常远小于d和k）$\n",
    "2. 冻结原始权重\n",
    "在训练过程中，原始权重$W$保持冻结状态，仅更新低秩矩阵$A$和$B$。这不仅减少了需要训练的参数，还保留了预训练模型的知识。\n",
    "3. 计算效率\n",
    "由于低秩矩阵的计算复杂度比完整权重矩阵低得多，LoRA显著提升了适配速度，同时降低了显存占用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f61a84-0bd2-4941-ae13-9da317193629",
   "metadata": {},
   "source": [
    "## 3.1 Low-Rank Matrix\n",
    "在线性代数中，低秩矩阵是指一个矩阵的秩（rank）比它的行数或列数要小得多。简单来说，它是一种结构化的矩阵，表示其数据`冗余性较高，很多信息可以用更少的自由度来表`示。\n",
    "\n",
    "矩阵的秩（Rank）: 矩阵的秩是线性无关行或列的数量。\n",
    "\n",
    "对于一个 $ m \\times n $ 的矩阵  A ：\n",
    "- 全秩矩阵：如果 $ \\text{rank}(A) = \\min(m, n) $，矩阵信息完整，行或列是线性无关的。\n",
    "- 低秩矩阵：如果 $ \\text{rank}(A) \\ll \\min(m, n) $，矩阵存在冗余，行或列是高度相关的。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4add762c-4fa3-4848-ba35-62b5b631ff20",
   "metadata": {},
   "source": [
    "低秩矩阵的实际例子\n",
    "\n",
    "1. 图像处理：\n",
    "在图像压缩中，图片的像素矩阵通常是近似低秩的，可以通过降维技术（如PCA）压缩数据。\n",
    "2. 推荐系统：\n",
    "用户-物品评分矩阵通常是稀疏的（大部分项为空），也可以近似为低秩矩阵，因为用户的评分行为通常可以归因于少数几个潜在特征（如偏好类型、价格敏感性等）。\n",
    "3. 自然语言处理：\n",
    "在语言模型中，大型权重矩阵可以近似为低秩形式，用来减少参数量并提升计算效率（例如LoRA方法）。\n",
    "低秩分解（Low-Rank Decomposition）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402cda96-24cd-45ff-a55c-d7317126d7dc",
   "metadata": {},
   "source": [
    "为了利用低秩矩阵的特性，我们可以将它分解成多个更小的矩阵：\n",
    "\n",
    "$$ A \\approx U \\cdot V^T $$\n",
    "\n",
    "- $A$ ：原始矩阵\n",
    "- $U$ ：表示特征的矩阵\n",
    "- $V^T$ ：表示组合权重的矩阵\n",
    "秩  $r$ ：通常 $ r \\ll \\min(m, n) $\n",
    "\n",
    "例如，通过奇异值分解（SVD）：\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "我们可以用较小的奇异值（低秩近似）来近似原始矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569136f6-138c-4cce-a621-c4d36e1f0114",
   "metadata": {},
   "source": [
    "![jupyter](./figures/LoRA1.png)\n",
    "\n",
    "固定$W_o$,引入$\\Delta W$,将$\\Delta W$拆成两个matrix $A,B$\n",
    "\n",
    "![jupyter](./figures/LoRA2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d0f88-21aa-4fd1-b51d-0f6b44cf280b",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Multimodal Model(多模态模型)\n",
    "\n",
    "多模态模型 (Multimodal Model) 是指能够同时处理和整合多种不同类型（模态）的数据的人工智能模型。模态是数据的类型或形式，例如：\n",
    "- 视觉模态：图像、视频\n",
    "- 语言模态：文本、语音\n",
    "- 感觉模态：触觉、气味\n",
    "- 其他模态：传感器数据、生物信号（如脑电波或心电图）\n",
    "\n",
    "通过整合多个模态的数据，模型可以更全面地理解和分析复杂的任务或场景。\n",
    "\n",
    "多模态模型的架构设计\n",
    "\n",
    "1.\t单模态特征提取：\n",
    "•\t每种模态使用专门的网络进行特征提取。例如：\n",
    "•\t图像：卷积神经网络(CNN)\n",
    "•\t文本：循环神经网络(RNN)或Transformer\n",
    "2.\t模态间融合：\n",
    "•\t提取的特征通过以下方式进行融合：\n",
    "•\t早期融合：在特征提取之前就合并不同模态的原始数据。\n",
    "•\t中期融合：单独提取特征后，将这些特征在某一层合并。\n",
    "•\t后期融合：各模态单独完成部分任务，再将结果整合。\n",
    "3.\t联合优化：\n",
    "•\t模型在训练时，目标函数能够优化所有模态之间的联合表现。\n",
    "\n",
    "经典多模态模型示例\n",
    "\n",
    "1.\tCLIP (Contrastive Language–Image Pretraining)：\n",
    "•\t由 OpenAI 开发，可以同时理解图像和文字，通过对比学习捕捉二者的语义关联。\n",
    "•\t应用于图像检索和跨模态理解。\n",
    "2.\tDALL·E：\n",
    "•\t基于文本生成图像的模型，将文字和视觉模态紧密结合。\n",
    "3.\tVideoBERT：\n",
    "•\t将语言模型扩展到视频分析，通过视频帧和字幕数据联合建模。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9a9b7-6f13-43c8-bd82-ee1493336318",
   "metadata": {},
   "source": [
    "# 5. Knowledge Distillation\n",
    "\n",
    "`一种模型压缩技术，通过将一个大模型(teacher model)的知识“提取”出来，传递给一个较小的模型(student model)，使student model能够以较小的规模实现接近teacher model的性能。`\n",
    "\n",
    "知识蒸馏的核心是利用教师模型的输出`概率分布`(soft targets)作为一种更丰富的监督信号，而不仅仅是使用训练数据的真实标签(hard targets）。这种额外的信息帮助学生模型更好地学习隐藏在数据中的模式。\n",
    "\n",
    "Soft Targets vs. Hard Targets\n",
    "\n",
    "•\tHard Targets：传统训练中，目标是实际的标签(如0或1)，即“某张图片是猫”。\n",
    "\n",
    "•\tSoft Targets：知识蒸馏中，目标是教师模型输出的概率分布(如“猫的概率是0.7，狗是0.2，兔子是0.1”)。\n",
    "\n",
    "•\tSoft targets 包含了关于类别之间关系的信息(比如“猫”和“狗”可能更相似，而“兔子”更不同)，从而更高效地指导学生模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c66422-2274-4b3d-9355-d6e8b42dc563",
   "metadata": {},
   "source": [
    "实现知识蒸馏的流程\n",
    "\n",
    "1.\t训练教师模型：\n",
    "\n",
    "先用大量数据和计算资源训练一个性能很高的复杂模型（如大型神经网络）。\n",
    "\n",
    "2.\t提取教师模型的知识：\n",
    "\n",
    "教师模型对训练数据进行预测，输出概率分布（soft targets）。\n",
    "\n",
    "4.\t训练学生模型：\n",
    "- 用 soft targets 和真实标签的组合，训练一个较小的学生模型。\n",
    "- 损失函数通常包括两部分：\n",
    "  1. 真实标签的交叉熵损失（Hard Loss）：学生模型必须对正确答案进行分类。\n",
    "  2. 蒸馏损失（Soft Loss）：学生模型需要匹配教师模型的输出分布。\n",
    "  \n",
    "公式示例：\n",
    "\n",
    "$\\mathcal{L} = \\alpha \\cdot \\mathcal{L}{\\text{hard}} + (1 - \\alpha) \\cdot \\mathcal{L}{\\text{soft}}$\n",
    "\n",
    "其中，$\\mathcal{L}_{\\text{soft}}$ 通常基于 KL 散度，$\\alpha$ 是调节权重的超参数。\n",
    "\n",
    "4.\t学生模型部署：\n",
    "经过训练的学生模型具有类似教师模型的性能，但规模更小，适合在资源受限的环境中运行。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d2372-c6e5-4740-9e54-f8d3f3529bd7",
   "metadata": {},
   "source": [
    "# 6. Bootstrapping\n",
    "Bootstrapping是一种用于改进模型性能或增强训练数据的方法。具体含义根据应用场景有所不同，但总体上指`模型在缺乏外部监督或资源的情况下，通过自身生成的数据或反馈`来改进自己。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99317aaa-fbfb-46f3-996f-fcd66a632ea8",
   "metadata": {},
   "source": [
    "应用\n",
    "1. 数据生成与增强\n",
    "\n",
    "在自然语言处理（NLP）或计算机视觉任务中，Bootstrapping 可以用来自动生成更多的训练数据或标注数据，以便增强模型能力。\n",
    "\n",
    "机制：\n",
    "\n",
    "\t•初始模型从有限的标注数据开始训练。\n",
    "\t•使用训练好的初始模型生成新的数据或标签（预测）。\n",
    "\t•将这些新生成的数据加入训练集中，继续训练模型。\n",
    "\n",
    "优点：\n",
    "\n",
    "\t•不依赖额外的人类标注，减少成本。\n",
    "\t•通过迭代逐步提升模型性能。\n",
    "\n",
    "示例：\n",
    "\n",
    "\t•在问答任务中，初始模型预测一些未标注问题的答案，将这些预测答案作为伪标签（pseudo-label）加入训练集。\n",
    "\n",
    "2. 自监督学习\n",
    "\n",
    "在自监督学习中，Bootstrapping 通常指模型通过生成自身的监督信号进行训练。例如，使用部分数据推测其缺失部分，或预测扰动数据的原始信息。\n",
    "\n",
    "机制：\n",
    "\n",
    "\t•通过设计任务（如填空、重建、对比学习）让模型学习数据内部的模式。\n",
    "\t•模型的输入和输出构成了训练数据，避免需要显式标签。\n",
    "\n",
    "示例：\n",
    "\n",
    "\t•BERT 使用遮蔽语言模型（Masked Language Model, MLM），让模型预测被随机遮蔽的词。\n",
    "\t•SimCLR 在对比学习中，将增强后的数据视为同类样本，模型通过对比学习优化表示。\n",
    "\n",
    "3. 知识蒸馏中的 Bootstrapping\n",
    "\n",
    "在知识蒸馏（Knowledge Distillation）中，Bootstrapping 用于让一个学生模型学习一个复杂教师模型的知识。\n",
    "\n",
    "机制：\n",
    "\n",
    "\t•教师模型首先通过外部数据训练，生成带有丰富上下文信息的 soft targets（如概率分布）。\n",
    "\t•学生模型用这些 soft targets 学习，从而获得教师模型的知识。\n",
    "\n",
    "ToolFormer 的实现：\n",
    "\n",
    "\t•教师模型（GPT-3.5 或更大的预训练模型）自己生成工具调用示例，并验证其有效性。\n",
    "\t•学生模型使用这些示例进行训练，学会如何调用工具，如搜索引擎或计算器。\n",
    "    \n",
    "4. 强化学习中的 Bootstrapping\n",
    "\n",
    "在强化学习中，Bootstrapping 是指模型利用当前估计的值函数来改进下一步的估计。\n",
    "\n",
    "机制：\n",
    "\n",
    "\t•模型通过试探性动作获得即时奖励，同时利用现有的值函数估计未来奖励。\n",
    "\t•使用当前估计的未来奖励更新策略或值函数，逐步改进模型。\n",
    "\n",
    "示例：\n",
    "\n",
    "\t•Q-learning：利用当前的 Q 值估计未来收益，更新 Q 值表。\n",
    "5. 自举验证（Bootstrapping for Validation）\n",
    "\n",
    "在统计学中，Bootstrapping 是一种重采样方法，用来估计模型的性能或分布特性。\n",
    "\n",
    "机制：\n",
    "\n",
    "\t•从现有数据中随机采样（有放回），生成多个采样数据集。\n",
    "\t•使用这些采样数据集训练模型，评估性能。\n",
    "\t•对性能指标（如平均误差、置信区间）进行统计估计。\n",
    "\n",
    "优点：\n",
    "\n",
    "\t•在数据量较小的情况下，提供了可靠的性能估计。\n",
    "\t•不需要额外的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7bb8e",
   "metadata": {},
   "source": [
    "# 7. Off-the-Shelf Language models\n",
    "\n",
    "Off-the-Shelf Language Models 是指`现成的、开箱即用的`语言模型。这些模型通常已经经过预训练，能够直接应用于各种任务，而不需要从零开始训练。这种术语在自然语言处理(NLP)领域中非常常见，用来描述那些预先训练好、可以直接使用或轻松适配的模型。\n",
    "\n",
    "主要特点:\n",
    "\n",
    "1.\t预训练好的模型：\n",
    "\tOff-the-shelf 模型已经在大规模数据集上完成了预训练，具备广泛的语言理解能力，例如 GPT、BERT、RoBERTa 等。\n",
    "2.\t无需从头训练：\n",
    "\t使用者只需对模型进行微调（fine-tuning）或直接输入特定任务的数据即可完成应用。\n",
    "3.\t通用性强：\n",
    "\t这些模型通常适用于各种任务，如文本分类、机器翻译、问答、文本生成等。\n",
    "4.\t易于访问：\n",
    "\t通常可以从开源平台（如 Hugging Face、TensorFlow Hub）或商业服务（如 OpenAI API）中直接获取。\n",
    "\n",
    "示例:\n",
    "\n",
    "1.\tGPT Models：\n",
    "\tGPT 系列（如 GPT-3）是典型的 off-the-shelf 模型，用户可以通过 API 调用生成自然语言文本。\n",
    "2.\tBERT：\n",
    "\tBERT 是一个预训练的 off-the-shelf 语言模型，可直接用于句子嵌入或语义相似度计算。\n",
    "3.\tHugging Face Transformers：\n",
    "\tHugging Face 提供了大量的 off-the-shelf 模型库，用户可以直接加载并使用模型进行各种 NLP 任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e124b-1847-43d8-b2f0-74ab2b9718ac",
   "metadata": {},
   "source": [
    "# 8.Ablation Study(消融研究)\n",
    "是深度学习研究中常用的一种分析方法，用于评估模型的不同组件或特性对整体性能的影响。通过逐步移除或替换模型中的某些部分，观察性能的变化，研究者可以确定哪些部分是必要的，哪些部分是次要的，甚至可能是不必要的。\n",
    "\n",
    "具体步骤\n",
    "\n",
    "1.\t确定模型组件：\n",
    "\t将模型分解为多个部分或特性，比如某些层、模块、损失函数、数据增强技术等。\n",
    "2.\t移除或替换：\n",
    "\t逐步移除某个组件，或用更简单的替代方法替换组件。\n",
    "3.\t观察影响：\n",
    "\t比较完整模型与移除某一组件后的性能变化（如准确率、F1 分数、损失值等）。\n",
    "4.\t总结贡献：\n",
    "\t如果移除某组件导致性能显著下降，则说明该组件对模型性能有重要贡献。\n",
    "\t如果移除后性能无明显变化，则可能该组件对任务的贡献较小，甚至可以考虑去掉以简化模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87aa43b-baa6-4552-96a6-3c59c57d2d92",
   "metadata": {},
   "source": [
    "# 9. Prefix Prompt 和 Suffix Prompt \n",
    "根据 prompt 在输入中的位置不同，可以分为 Prefix Prompt 和 Suffix Prompt\n",
    "\n",
    "\n",
    "|特性|Prefix Prompt|Prefix Prompt|\n",
    "|---|---|---|\n",
    "|位置|在输入文本的前面|\t在输入文本的后面|\n",
    "|作用|\t提供任务背景和指导|对模型输出添加补充要求或约束|\n",
    "|适用场景|提前设定任务类型、描述背景|约束生成格式、引导特定的输出方向|\n",
    "|示例|“Translate the following text: “ + 用户输入文本|\t用户输入文本 + “Translate it into French.”|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d528b6f-e358-434e-83a0-99c8d5a6d00c",
   "metadata": {},
   "source": [
    "# 10. Knowledge Graph (KG， 知识图谱)\n",
    "\n",
    "是一种以图结构表示知识的形式，主要由实体（nodes）和关系（edges）组成，用于存储和组织语义化的知识。知识图谱的目标是通过结构化数据直观地描述和管理实体之间的关系和属性。\n",
    "\n",
    "主要功能\n",
    "\n",
    "1. 存储和查询知识：结构化存储知识，使得查询更加高效\n",
    "2. 语义推理：利用逻辑规则推导隐藏的知识。\n",
    "3. 支持问答系统：在用户提问时，快速找到答案或相关信息。\n",
    "\n",
    "应用场景\n",
    "\n",
    "1.\t问答系统（QA）：\n",
    "\t比如，通过查询知识图谱，回答“法国的首都是哪里？”。\n",
    "\t知识图谱可以直接返回实体 “Paris”。\n",
    "2.\t推荐系统：\n",
    "\t根据用户偏好生成推荐。例如，基于知识图谱推荐相似电影或书籍。\n",
    "3.\t搜索引擎：\n",
    "\tGoogle 的“知识卡片”即基于知识图谱，显示相关实体和信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43519c2-4e9c-4445-a680-a5da4027d301",
   "metadata": {},
   "source": [
    "# 11. Vector Retrieval (向量检索)\n",
    "是一种基于向量化数据的检索，通常用于处理非结构化数据（如文本/图像/视频）以及高维嵌入表示的快速查询\n",
    "\n",
    "核心思想\n",
    "\n",
    "\t将文本、图像或其他对象通过模型转换为高维向量表示（embedding）。\n",
    "\t在嵌入空间中，通过计算向量间的相似度（如余弦相似度、欧几里得距离）找到最相关的结果。\n",
    "\n",
    "关键步骤\n",
    "\n",
    "\t1.向量化表示：\n",
    "    \t使用深度学习模型（如 BERT、CLIP）将输入数据（如文本或图像）转化为固定维度的向量。\n",
    "    \t例如：句子 \"What is the capital of France?\" 被嵌入为一个 768 维的向量。\n",
    "\t2.索引构建：\n",
    "    \t将所有向量存储在一个索引结构（如 Faiss、HNSW）中，以便快速检索。\n",
    "\t3.相似度搜索：\n",
    "    \t给定查询向量，通过计算查询向量与索引中所有向量的相似度，返回最匹配的结果。\n",
    "\n",
    "主要功能\n",
    "\n",
    "\t1. 高效处理大规模非结构化数据。\n",
    "\t2. 在海量数据中快速找到相关的对象。\n",
    "\n",
    "应用场景\n",
    "\n",
    "\t1.语义检索：\n",
    "    \t搜索与输入文本语义相似的文档或句子，而不仅仅依赖关键词匹配。\n",
    "    \t示例：用户搜索“法国首都”，系统检索到描述“巴黎是法国首都”的句子。\n",
    "\t2.多模态搜索：\n",
    "\t\t使用跨模态嵌入（如文本与图像），实现从文本中搜索图片，或者从图片中搜索相关文本。\n",
    "\t3.推荐系统：\n",
    "\t\t基于用户行为或偏好向量，检索与用户兴趣最相似的内容。\n",
    "\t4.问答系统：\n",
    "\t\t在嵌入空间中找到与用户问题最匹配的知识或答案。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dfe5d4-f3ba-436d-b850-78d1ebc345de",
   "metadata": {},
   "source": [
    "# 12. DIKW\n",
    "在深度学习和数据科学中，DIKW代表数据（Data）、信息（Information）、知识（Knowledge）和智慧（Wisdom）的层次结构。这是一个模型，用来解释数据如何被转化为信息，信息如何进一步转化为知识，以及如何最终形成智慧。这个概念强调了从大量数据中提取有用信息和知识的过程，是信息管理和知识管理领域中的一个重要理论。\n",
    "\n",
    "\t•\t数据（Data）：原始数据，是没有经过处理的事实和数字。\n",
    "\t•\t信息（Information）：处理或组织后的数据，提供了更多的语境，使数据更有意义。\n",
    "\t•\t知识（Knowledge）：通过分析信息得到的，能够用来做决策的洞察。\n",
    "\t•\t智慧（Wisdom）：深入的理解，使个体能够识别和解决核心问题，进行明智的判断和决策。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944ecf7a-77b3-4333-8f25-bd7550c8cb37",
   "metadata": {},
   "source": [
    "# 13. Subject-predicate-object(SPO)\n",
    "Subject-Predicate-Object (SPO) 是自然语言处理（NLP）中用来描述句子结构的一种基本形式，特别是在语义分析和信息提取中。这种模式有助于理解和表示句子中的基本意义结构。\n",
    "\n",
    "\t•\t主语（Subject）：句子的主语通常是执行动作的人或事物。\n",
    "\t•\t谓语（Predicate）：谓语通常是描述主语的动作或状态的动词。\n",
    "\t•\t宾语（Object）：宾语是受到谓语动作影响的人或事物。\n",
    "\n",
    "例如，在句子 “The cat (subject) chased (predicate) the mouse (object).” 中，“The cat” 是执行动作的主体，“chased” 是描述这个动作的谓语，而 “the mouse” 是这个动作的接收者，即宾语。\n",
    "\n",
    "在自然语言处理中，提取 SPO 结构有助于机器理解句子的语义内容，这对于任务如信息提取、知识图谱的构建、文本摘要等都是非常重要的。SPO 提取通常涉及依存句法分析和命名实体识别等技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d36ce-8d8b-4ff8-aae9-0ecd020b3c04",
   "metadata": {},
   "source": [
    "# 14.Semantic reasoning\n",
    "Semantic reasoning in the context of artificial intelligence and natural language processing (NLP) involves the ability of a system to understand and manipulate knowledge about the world as expressed through language. It requires an understanding of the meanings of words and sentences, and the relationships between them, beyond simple text patterns or statistical co-occurrences. This capacity enables the system to make inferences, draw conclusions, and provide more contextually relevant responses.\n",
    "\n",
    "Key Components of Semantic Reasoning:\n",
    "\n",
    "1. Knowledge Representation: This involves structuring real-world knowledge in a format that a machine can understand. Common methods include ontologies, semantic networks, and knowledge graphs which organize information in nodes and edges, representing entities and their interrelationships.\n",
    "2. Context Understanding: The ability to comprehend the situational and broader textual context in which words and phrases are used. This helps in determining the precise meaning in ambiguous scenarios, such as distinguishing between homonyms based on context.\n",
    "3. Inference Making: Using logical rules and learned patterns to derive conclusions that are not explicitly stated in the text. This involves reasoning over the knowledge base or directly from text using methods like rule-based logic, probabilistic reasoning, or neural network models.\n",
    "4. Semantic Parsing: Transforming natural language into an interpretable structured format that aligns with the system’s knowledge representation. Semantic parsers help in identifying the relationships between entities in a sentence, such as subject, predicate, and object (SPO).\n",
    "5. Commonsense Reasoning: The ability to apply everyday knowledge and practical understanding of how the world works, which is often taken for granted by humans, to enhance interpretations and predictions made by AI systems.\n",
    "\n",
    "Applications of Semantic Reasoning:\n",
    "\n",
    "1. Question Answering Systems: Enhancing the ability of AI to understand questions and provide accurate answers by inferring information that is not explicitly mentioned.\n",
    "2. Dialogue Systems: Enabling more natural interactions with users by understanding the semantics of the conversation and maintaining context over multiple turns.\n",
    "3. Information Extraction and Retrieval: Improving the precision of extracting relevant facts and details from large volumes of text and efficiently retrieving information based on semantic queries.\n",
    "4. Automated Reasoning and Decision Support: Facilitating complex decision-making processes by simulating human-like reasoning, aiding in scenarios like medical diagnosis, legal analysis, and financial planning.\n",
    "\n",
    "Semantic reasoning represents a significant shift from keyword-based approaches to a more nuanced understanding of language, making AI systems appear more intelligent and capable of handling complex human language interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d45c1-157e-46d3-b514-4a5100aacba4",
   "metadata": {},
   "source": [
    "# 15. Stochastic Policy Gradient (随机策略梯度)\n",
    "在强化学习中，随机策略梯度（Stochastic Policy Gradient） 是一种策略优化技术，主要用于优化代理（agent）的策略，使其在给定环境中获取尽可能多的奖励。这种方法直接对策略进行参数化，并使用梯度上升来优化策略参数，从而找到最大化累计奖励的策略。\n",
    "\n",
    "工作原理\n",
    "\n",
    "随机策略梯度方法依赖于策略函数$\\pi_\\theta(a|s)$，该函数定义了在给定状态$s$下采取动作$a$的概率，其中$\\theta$表示策略参数。这种策略是随机的，因为对于给定的状态，它提供了多个可能动作的概率分布。\n",
    "\n",
    "训练过程\n",
    "\n",
    "训练过程中，代理根据当前策略在环境中执行动作，并接收环境提供的反馈（奖励和新的状态）。然后，利用这些信息计算策略梯度，并使用梯度上升（或称为梯度下降的反向）来更新策略参数。策略梯度的一个核心组成是奖励函数，它评估采取特定动作后得到的长期奖励。\n",
    "\n",
    "数学表示\n",
    "\n",
    "随机策略梯度的数学表达式通常如下：\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}{\\pi\\theta}\\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_t \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\right]$$\n",
    "\n",
    "其中，$J(\\theta) $是目标函数，代表总奖励的期望值；$\\gamma$是折扣因子，用于平衡即时奖励和未来奖励；$R_t$是在时间$t$获得的奖励；$\\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) $是对策略函数的概率对数的梯度。\n",
    "\n",
    "算法和应用\n",
    "\n",
    "- REINFORCE：这是最基本的随机策略梯度算法，依赖于完整的轨迹来更新策略。\n",
    "- Actor-Critic：这是一种更高级的变体，其中包含两部分：执行者（actor）用于确定动作，批评者（critic）用于估算状态值函数。这种方法可以减少方差并加速学习。\n",
    "\n",
    "随机策略梯度方法在机器人控制、游戏玩法、资源管理等领域都有广泛的应用，特别是在那些需要连续动作空间的场景中非常有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92ea42b-443e-426d-8860-60a34f75ef9c",
   "metadata": {},
   "source": [
    "# 16. Adam\n",
    "\n",
    "optimizer=adam 是在深度学习模型中指定使用 Adam 优化器 的方式，通常在使用框架如 TensorFlow 或 PyTorch 构建和训练模型时会出现。Adam 是一种优化算法，它在训练过程中用来更新模型的权重，使损失函数逐步降低，从而提高模型的性能。\n",
    "\n",
    "`Adam 优化器的特点`\n",
    "\n",
    "Adam (Adaptive Moment Estimation) 是一种非常流行的优化算法，结合了 动量方法 (Momentum) 和 RMSProp 的优点。具体来说，它具有以下特点：\n",
    "1. 自适应学习率：\n",
    "\t\n",
    "    Adam 会根据每个参数的梯度变化情况动态调整学习率。\n",
    "\n",
    "\t它使用一阶动量 (梯度的均值) 和二阶动量 (梯度平方的均值) 来计算参数的更新。\n",
    " \n",
    "3. 快速收敛：\n",
    "\t相较于传统的梯度下降方法，Adam 优化器能更快地收敛到最优解，尤其在稀疏梯度和高维空间中表现良好。\n",
    "4. 鲁棒性：\n",
    "\t对超参数（如学习率）不太敏感，默认参数通常能提供较好的结果。\n",
    "\n",
    "`Adam 的公式`\n",
    "\n",
    "Adam 的更新规则基于以下几步：\n",
    "1. 计算梯度的一阶动量 m_t（梯度的均值）：\n",
    "\n",
    "$$m_t = \\beta_1 \\cdot m_{t-1} + (1 - \\beta_1) \\cdot g_t$$\n",
    "\n",
    "   $g_t$ 是当前梯度\n",
    "\n",
    "$\\beta_1$ 是一阶动量的指数衰减率（默认值为 0.9）\n",
    "    \n",
    "2. 计算梯度的二阶动量 v_t（梯度平方的均值）：\n",
    "\n",
    "$$v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot g_t^2$$\n",
    "\n",
    "   $\\beta_2$ 是二阶动量的指数衰减率（默认值为 0.999）\n",
    "    \n",
    "3. 偏差校正（因为 $m_t$ 和 $v_t$ 初始值为 0，会导致偏差）：\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "4. 更新参数：\n",
    "\n",
    "$$\\theta_t = \\theta_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "   $\\eta$ 是学习率\n",
    "\n",
    "   $\\epsilon$ 是一个非常小的值，用于防止除以零（默认值为 10^{-7}）。\n",
    "\n",
    "\n",
    "`使用方式`\n",
    "\n",
    "在 TensorFlow 中，可以通过如下代码指定 optimizer=adam：\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "在 PyTorch 中，可以这样使用：\n",
    "```python\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "`参数设置`\n",
    "\n",
    "- 学习率 (learning_rate)：默认是 0.001，但可以根据问题调整。\n",
    "- $\\beta_1$ 和 $\\beta_2$：默认分别是 0.9 和 0.999。\n",
    "- $\\epsilon$：默认是 $10^{-7}$。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58367b3-60bf-4ea8-8ec5-38980b3971de",
   "metadata": {},
   "source": [
    "# 17. LSTM\n",
    "LSTM (Long Short-Term Memory) 是一种特殊的 循环神经网络（RNN, Recurrent Neural Network） 架构，用于处理和学习时间序列数据或序列任务中的长期依赖关系。它由 Hochreiter 和 Schmidhuber 在 1997 年提出，并在 NLP（自然语言处理）、语音识别、时间序列预测等领域广泛应用。\n",
    "\n",
    "普通 RNN 在处理长序列时，会因为 梯度消失（vanishing gradients） 或 梯度爆炸（exploding gradients） 问题而难以捕捉远距离依赖信息。LSTM 通过引入一种特殊的结构（记忆单元和门机制）来缓解这一问题，能够有效记住较长时间范围内的重要信息。\n",
    "\n",
    "`LSTM的结构`\n",
    "\n",
    "LSTM的核心是记忆单元（Memory Cell）和门机制（Gate Mechanisms）。它的每一个时间步都会接收当前输入和前一个时间步的信息，并通过以下门控单元控制哪些信息保留或遗忘：\n",
    "1. 遗忘门（Forget Gate）:\n",
    "\t决定当前时间步的输入信息中，有哪些需要从记忆单元中遗忘。\n",
    "\t公式：\n",
    "\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "$f_t$：遗忘门的输出（范围在 0 和 1 之间）\n",
    "\n",
    "$\\sigma$：激活函数 Sigmoid\n",
    "\n",
    "2. 输入门（Input Gate）:\n",
    "\t决定当前时间步的输入信息中，有哪些需要写入记忆单元。\n",
    "\t包含两部分：\n",
    "\t1.\t候选记忆值：$\\tilde{C}t = \\tanh(W_c \\cdot [h{t-1}, x_t] + b_c)$\n",
    "\n",
    "\t2.\t输入门的激活值：$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "\t3.\t更新记忆单元：$C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t$\n",
    "\n",
    "3. 输出门（Output Gate）:\n",
    "\t决定当前时间步哪些信息需要输出。\n",
    "\t公式：$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "\n",
    "\t输出的隐藏状态：\n",
    "\n",
    "$$h_t = o_t \\cdot \\tanh(C_t)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd69ef-821f-4ff1-aef5-8de2403392a0",
   "metadata": {},
   "source": [
    "数据流\n",
    "\n",
    "1.\t接收当前输入 $x_t$ 和前一个时间步的隐藏状态 $h_{t-1}$。\n",
    "2.\t通过门机制（遗忘门、输入门、输出门）处理输入，更新记忆单元状态 $C_t$ 和隐藏状态 $h_t$。\n",
    "3.\t输出当前时间步的隐藏状态 $h_t$，传递到下一时间步。\n",
    "\n",
    "\n",
    "LSTM 的优点\n",
    "\n",
    "1.\t解决长程依赖问题：\n",
    "\t通过记忆单元和门机制，可以在较长时间范围内保留重要信息。\n",
    "2.\t避免梯度消失：\n",
    "\t通过遗忘门和记忆单元的加法操作，梯度可以在时间步中流动而不衰减。\n",
    "3.\t适用多种序列任务：\n",
    "\t可用于序列分类、序列生成、序列预测等任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aff12e-d164-4a57-a630-39837741adce",
   "metadata": {},
   "source": [
    "在 TensorFlow/Keras 中：\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# 定义一个简单的 LSTM 模型\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(10, 20)),  # 输入序列长度为10，特征数为20\n",
    "    Dense(1, activation='sigmoid')   # 输出层\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815574e-0aa6-47bd-a361-ff88ec42402a",
   "metadata": {},
   "source": [
    "在 PyTorch 中：\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, (h_n, c_n) = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return out\n",
    "\n",
    "# 定义模型\n",
    "model = LSTMModel(input_size=20, hidden_size=50, output_size=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec8f69-d784-41f4-9258-a4f8fe90441f",
   "metadata": {},
   "source": [
    "# 18. GloVe\n",
    "GloVe (Global Vectors for Word Representation) 是一种用于生成词向量（Word Embeddings）的方法，由斯坦福大学研究团队在 2014 年提出。它是一种无监督学习方法，可以将单词表示为固定长度的连续向量，并捕捉单词之间的语义关系和上下文信息。\n",
    "\n",
    "`GloVe 的主要特点`\n",
    "\n",
    "1.\t词嵌入（Word Embedding）：\n",
    "\tGloVe 将每个单词映射到一个连续的低维向量空间中。\n",
    "\t单词的语义信息通过向量的几何关系（如距离、方向）进行表示。\n",
    "2.\t结合统计信息和局部上下文：\n",
    "\t与 Word2Vec 不同，GloVe 不直接基于上下文窗口，而是利用整个语料库中单词共现矩阵的统计信息，生成词向量。\n",
    "3.\t捕捉单词关系：\n",
    "\tGloVe 的词向量能捕捉到语义和语法上的关系，比如：\n",
    "\t- 向量关系：$\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$\n",
    "\t- 相似性关系：$\\text{Paris}$ 和$ \\text{France} $的距离与$ \\text{Berlin} 和 \\text{Germany} $的距离相似。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff48c0-9502-49b4-8bac-19018fd6933a",
   "metadata": {},
   "source": [
    "`GloVe 的核心思想`\n",
    "\n",
    "GloVe 通过构建一个单词共现矩阵，捕捉单词间的全局统计信息。具体过程如下：\n",
    "\n",
    "1.\t单词共现矩阵：\n",
    "\t- 在构建词嵌入前，统计整个语料库中单词之间的共现频率，形成一个矩阵 $X$。\n",
    "\t- $X_{ij}$：单词 $i$ 和 $j$ 在一定上下文窗口内共现的次数。\n",
    "\n",
    "2.\t优化目标：\n",
    "\t- GloVe 的核心目标是通过优化，使词向量的点积可以近似表示单词共现频率：$$w_i^T w_j + b_i + b_j = \\log(X_{ij})$$\n",
    "    - $w_i$, $w_j$：单词 $i$ 和 $j$ 的词向量。\n",
    "\t- $b_i, b_j$：偏置项。\n",
    "\t- $X_{ij}$：单词 $i$ 和 $j$ 的共现频率。\n",
    "\n",
    "3.\t损失函数：\n",
    "\t- GloVe 使用了加权的平方误差损失函数来优化上述公式：$$J = \\sum_{i,j} f(X_{ij}) \\cdot \\left(w_i^T w_j + b_i + b_j - \\log(X_{ij})\\right)^2$$\n",
    "\n",
    "\t- $f(X_{ij})$ 是一个加权函数，用于减小低频共现单词对的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703d597-20c0-4183-8edd-6c71c69f3cdc",
   "metadata": {},
   "source": [
    "`使用 GloVe`\n",
    "\n",
    "1. 预训练的 GloVe 词向量\n",
    "\n",
    "斯坦福团队提供了多种预训练的 GloVe 词向量模型，可以直接下载并用于自己的项目。这些模型基于不同规模的语料库（如 Wikipedia 和 Common Crawl）训练。\n",
    "\n",
    "- 例如：\n",
    "- 维度：50, 100, 200, 300（词向量的维度越高，表示能力越强，但计算成本也越高）。\n",
    "- 语料库大小：6B、42B、840B（不同规模的语料库，影响模型质量）。\n",
    "\n",
    "预训练词向量下载地址：[GloVe 官方网站](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "2. 使用 GloVe 的预训练向量\n",
    "以 Python 为例，加载 GloVe 词向量：\n",
    "```python\n",
    "# 加载预训练的 GloVe 词向量\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "# 查询单词的词向量\n",
    "word = 'king'\n",
    "vector = embeddings_index.get(word)\n",
    "print(f\"The embedding for '{word}': {vector}\")\n",
    "```\n",
    "3. 在深度学习中使用 GloVe\n",
    "\n",
    "可以将 GloVe 嵌入向量加载到模型的嵌入层中。例如，在 TensorFlow/Keras 中：\n",
    "```python\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 构建嵌入矩阵\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 定义嵌入层\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix], trainable=False)\n",
    "\n",
    "# 创建模型\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(...)  # 添加其他层\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20147e3d-d6ac-47ec-bb53-42da0461b213",
   "metadata": {},
   "source": [
    "GloVe 的局限性\n",
    "\n",
    "1.\t静态嵌入：\n",
    "\tGloVe 的词向量是静态的，所有单词的向量在训练后是固定的，对多义词的处理能力较差。\n",
    "2.\t需要大量语料：\n",
    "\tGloVe 的效果依赖于大量的高质量语料进行训练。\n",
    "3.\t训练复杂度高：\n",
    "\t构建共现矩阵的内存开销较大，对大规模词汇表可能不适用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119c5a4-eeed-4d54-92b1-03c2e3fef932",
   "metadata": {},
   "source": [
    "# 19. Self-supervisded learning\n",
    "Self-supervised learning（自监督学习） 是一种机器学习方法，属于无监督学习的一种特殊形式。它通过从数据本身生成标签（伪标签），利用这些伪标签来训练模型，从而学到有效的表示。自监督学习不需要人工标注数据，因此可以充分利用海量的无标签数据。\n",
    "\n",
    "`自监督学习的核心思想`\n",
    "\n",
    "自监督学习的核心在于设计一种预训练任务（pretext task），用原始数据生成伪标签，并通过预测这些伪标签来训练模型。目标是让模型通过解决这个预训练任务，学习到数据的结构信息或特征表示。\n",
    "\n",
    "这些学习到的特征可以在下游任务（如分类、检测等）中迁移使用，从而减少对标注数据的依赖。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3009bd-0711-41c6-ab5a-ff2f4d479381",
   "metadata": {},
   "source": [
    "# 20. Logits\n",
    "在深度学习中，logits 是指模型输出的未归一化的预测值。这些值通常是神经网络的最后一层输出，还未经过激活函数（如 Softmax 或 Sigmoid）的处理。计算 logits 通常是模型推理过程中不可或缺的一步，特别是在分类任务中。\n",
    "\n",
    "\n",
    "`Logits 的数学表示`\n",
    "\n",
    "假设神经网络的最后一层是一个全连接层，输出为一个向量 \\mathbf{z} = [z_1, z_2, \\dots, z_n]，每个 z_i 是 logits：\n",
    "- Logits 是网络的直接输出：\n",
    "\n",
    "$z_i = W_i \\cdot x + b_i$\n",
    "\n",
    "其中$ W_i$ 是权重，$x$ 是输入，$b_i$ 是偏置。\n",
    "\n",
    "- 在多分类任务中，logits 通常通过 Softmax 转换为概率：\n",
    "\n",
    "$p_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{n} \\exp(z_j)}$\n",
    "\n",
    "这里 $p_i$ 是类别 $i$ 的预测概率。\n",
    "\n",
    "- 在二分类任务中，logits 通常通过 Sigmoid 转换为概率：\n",
    "\n",
    "$p = \\frac{1}{1 + \\exp(-z)}$\n",
    "\n",
    "这里 p 是正类的预测概率。\n",
    "\n",
    "\n",
    "\n",
    "`Logits 与激活函数的关系`\n",
    "\n",
    "1. 多分类任务：\n",
    "\n",
    "- Logits 是 Softmax 的输入：\n",
    "\n",
    "$\\text{Softmax}(\\mathbf{z})i = \\frac{\\exp(z_i)}{\\sum{j=1}^n \\exp(z_j)}$\n",
    "\n",
    "- Logits 的维度与类别数相同。\n",
    "\n",
    "\n",
    "2. 二分类任务：\n",
    "\t\n",
    "- Logits 是 Sigmoid 的输入：\n",
    "\n",
    "$\\sigma(z) = \\frac{1}{1 + \\exp(-z)}$\n",
    "\n",
    "- Logits 通常是一个标量或一维向量。\n",
    "\n",
    "\n",
    "`Logits 的计算步骤`\n",
    "\n",
    "以分类任务为例，假设模型有一个全连接层输出 logits：\n",
    "对于一个输入 x：\n",
    "1. 计算 logits：\n",
    "\n",
    "$\\mathbf{z} = W \\cdot x + b$\n",
    "\n",
    "这里 W 是权重矩阵，b 是偏置向量。\n",
    "\t\n",
    "2. 计算损失（如交叉熵损失）：\n",
    "\n",
    "- 多分类：\n",
    "\n",
    "$\\text{CrossEntropyLoss} = - \\sum_{i=1}^n y_i \\cdot \\log\\left(\\frac{\\exp(z_i)}{\\sum_{j=1}^n \\exp(z_j)}\\right)$\n",
    "\n",
    "在 PyTorch 中，这一步可以直接用 logits 计算：\n",
    "```python\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, labels)\n",
    "```\n",
    "\n",
    "- 二分类：\n",
    "\n",
    "$\\text{Binary CrossEntropyLoss} = - \\left( y \\cdot \\log(\\sigma(z)) + (1 - y) \\cdot \\log(1 - \\sigma(z)) \\right)$\n",
    "\n",
    "在 PyTorch 中可以用 BCEWithLogitsLoss 直接处理 logits：\n",
    "```python\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(logits, labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18154a36-641a-4bc2-9f29-48b30547144a",
   "metadata": {},
   "source": [
    "`Logits 的代码示例`\n",
    "\n",
    "1. 在 TensorFlow 中\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 模型的最后一层输出 logits\n",
    "logits = tf.keras.layers.Dense(10)(inputs)  # 假设有 10 个类别\n",
    "\n",
    "# 计算 Softmax 概率\n",
    "probs = tf.nn.softmax(logits)\n",
    "\n",
    "# 直接计算交叉熵损失（内含 logits）\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=true_labels, logits=logits)\n",
    "```\n",
    "\n",
    "2. 在 PyTorch 中\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模型的最后一层输出 logits\n",
    "logits = torch.randn(32, 10)  # 假设 batch_size=32, 类别数=10\n",
    "\n",
    "# 计算 Softmax 概率\n",
    "probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# 使用 CrossEntropyLoss（直接处理 logits）\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, true_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f894a59-e394-447c-8e45-0b3baea8e94b",
   "metadata": {},
   "source": [
    "# 21. GRU\n",
    "GRU (Gated Recurrent Unit) 是一种循环神经网络（RNN）变体，由 Cho 等人于 2014 年提出。它是一种轻量级的替代 LSTM（Long Short-Term Memory），用于解决标准 RNN 中的梯度消失和梯度爆炸问题，特别适合处理序列数据。\n",
    "\n",
    "GRU 是一种门控循环单元，通过引入门控机制动态控制信息流，从而保留长期依赖信息并忽略不必要的历史信息。与 LSTM 类似，它可以有效地捕捉序列数据中的长期依赖关系，但结构更简单。\n",
    "\n",
    "`GRU 的结构`\n",
    "\n",
    "GRU 的结构比 LSTM 更简单，主要包含两个门：更新门和重置门。\n",
    "\n",
    "1. `更新门` ($z_t$):\n",
    "\t\n",
    "控制当前时间步的信息有多少需要从过去传递下来，以及有多少需要从当前输入中更新。\n",
    "   \n",
    "更新门的公式：\n",
    "\n",
    "$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)$\n",
    "\n",
    "$W_z$：权重矩阵\n",
    "\n",
    "$h_{t-1}$：上一时间步的隐藏状态\n",
    "\n",
    "$x_t$：当前时间步的输入\n",
    "\n",
    "$\\sigma$：Sigmoid 激活函数\n",
    "\n",
    "2. `重置门` ($r_t$):\n",
    "\n",
    "控制是否丢弃上一时间步隐藏状态的信息，专注于当前时间步的信息。\n",
    "\n",
    "重置门的公式：\n",
    "\n",
    "$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)$\n",
    "\n",
    "3. `候选隐藏状态` ($\\tilde{h}_t$):\n",
    "\n",
    "计算当前时间步的新候选隐藏状态。\n",
    "\n",
    "公式：\n",
    "\n",
    "$\\tilde{h}t = \\tanh(W_h \\cdot [r_t \\odot h{t-1}, x_t] + b_h)$\n",
    "\n",
    "$r_t \\odot h_{t-1}$：重置门对上一时间步隐藏状态的作用（逐元素相乘）。\n",
    "\n",
    "4. `隐藏状态更新`:\n",
    "\t\n",
    "使用更新门将候选隐藏状态和上一时间步的隐藏状态组合：\n",
    "\n",
    "$h_t = z_t \\odot h_{t-1} + (1 - z_t) \\odot \\tilde{h}_t$\n",
    "\n",
    "$z_t$：控制保持多少旧信息。\n",
    "\n",
    "$1 - z_t$：控制加入多少新信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2413f33-254e-4e9a-9cb8-268148bcd236",
   "metadata": {},
   "source": [
    "`GRU与LSTM 的对比`\n",
    "\n",
    "|特性|GRU|LSTM|\n",
    "|---|---|---|\n",
    "|门的数量|2（更新门、重置门）|3（输入门、遗忘门、输出门）|\n",
    "|单元的复杂度|较简单|较复杂|\n",
    "|训练时间|较快|较慢|\n",
    "|表现|在某些任务中接近甚至优于LSTM|在捕捉长期依赖时通常更稳定|\n",
    "|内存使用|较少|较多|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70a19c1-e356-4eae-bc39-c52e1b8eb1e2",
   "metadata": {},
   "source": [
    "`GRU 的代码示例`\n",
    "\n",
    "1. 在 PyTorch 中使用 GRU\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义 GRU 模型\n",
    "gru = nn.GRU(input_size=10, hidden_size=20, num_layers=1, batch_first=True)\n",
    "\n",
    "# 输入数据 (batch_size=32, sequence_length=5, input_size=10)\n",
    "inputs = torch.randn(32, 5, 10)\n",
    "h_0 = torch.zeros(1, 32, 20)  # 初始隐藏状态 (num_layers=1, batch_size=32, hidden_size=20)\n",
    "\n",
    "# 前向传播\n",
    "outputs, h_n = gru(inputs, h_0)\n",
    "print(outputs.shape)  # 输出的隐藏状态 (batch_size=32, sequence_length=5, hidden_size=20)\n",
    "print(h_n.shape)      # 最后一时间步的隐藏状态 (num_layers=1, batch_size=32, hidden_size=20)\n",
    "```\n",
    "\n",
    "\n",
    "2. 在 TensorFlow/Keras 中使用 GRU\n",
    "```python\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 定义 GRU 模型\n",
    "model = Sequential([\n",
    "    GRU(20, input_shape=(5, 10), return_sequences=True),\n",
    "    Dense(1, activation='sigmoid')  # 输出层\n",
    "])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f4d2ef-38ca-429b-b4d2-ecc2c3c231a3",
   "metadata": {},
   "source": [
    "# 22.Neo4j\n",
    "是一种图数据库管理系统，用于存储和管理图形结构化数据，以支持高效的关系分析和复杂查询。它是业界领先的图数据库，以其灵活性、速度和强大的关系建模能力著称。\n",
    "\n",
    "`Neo4j 的核心概念`\n",
    "\n",
    "1.\t节点（Nodes）：\n",
    "- 数据的基本单元，表示实体。\n",
    "- 每个节点可以带有一个或多个标签（labels）来分类，比如 Person、Product。\n",
    "```python\n",
    "(n:Person {name: 'Alice', age: 25})\n",
    "```\n",
    "\n",
    "2. 关系（Relationships）：\n",
    "- 表示节点之间的连接关系，具有方向性（单向或双向）。\n",
    "- 关系也可以有属性。\n",
    "```python\n",
    "(a)-[:FRIEND]->(b)\n",
    "```\n",
    "\n",
    "3. 属性（Properties）：\n",
    "- 存储在节点或关系上的键值对，用于描述实体或连接的细节。\n",
    "```python\n",
    "(n:Person {name: 'Bob', age: 30})\n",
    "(n)-[:WORKS_AT {since: 2015}]->(m:Company {name: 'Neo4j'})\n",
    "```\n",
    "\n",
    "4. 图（Graph）：\n",
    "- 由节点和关系组成的图结构，是 Neo4j 数据存储和查询的核心。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6585c8-f3c6-49bd-9e5c-b6f30d3cc5f8",
   "metadata": {},
   "source": [
    "`Neo4j 的基本操作示例`\n",
    "\n",
    "- 创建节点和关系\n",
    "\n",
    "```cypher\n",
    "CREATE (alice:Person {name: 'Alice', age: 25})\n",
    "CREATE (bob:Person {name: 'Bob', age: 30})\n",
    "CREATE (neo:Company {name: 'Neo4j'})\n",
    "CREATE (alice)-[:FRIEND]->(bob)\n",
    "CREATE (bob)-[:WORKS_AT]->(neo)\n",
    "```\n",
    "\n",
    "- 查找所有 Person 节点：\n",
    "```cypher\n",
    "MATCH (p:Person) RETURN p\n",
    "```\n",
    "\n",
    "- 查找 Alice 的朋友：\n",
    "```cypher\n",
    "MATCH (a:Person {name: 'Alice'})-[:FRIEND]->(friend) RETURN friend\n",
    "```\n",
    "\n",
    "- 查找 Bob 工作的公司：\n",
    "```cypher\n",
    "MATCH (b:Person {name: 'Bob'})-[:WORKS_AT]->(company) RETURN company\n",
    "```\n",
    "\n",
    "- 查找两个节点之间的所有路径：\n",
    "```cypher\n",
    "MATCH p = (a:Person)-[*]->(b:Company) RETURN p\n",
    "```\n",
    "\n",
    "### Cypher是一种专门为图数据库（如Neo4j）设计的图查询语言，用于操作和查询存储在知识图谱（Knowledge Graph）中的数据。它类似于关系型数据库中的 SQL，但针对图数据进行了优化，更适合表示和操作节点（Node）、关系（Relationship）和属性（Property）的结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d45e1-6d7c-4066-8b09-c22ff9063930",
   "metadata": {},
   "source": [
    "# 23. Azure \n",
    "是由微软（Microsoft）提供的一组云计算服务平台，正式名称是 Microsoft Azure。它为用户提供了一整套云计算功能，包括计算、存储、网络、人工智能、分析等服务，帮助开发者、企业和组织快速构建、部署和管理应用程序。\n",
    "\n",
    "Azure 是当前主流的云计算平台之一，与 AWS（Amazon Web Services） 和 Google Cloud Platform（GCP） 并列为三大云服务提供商。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1578122-cfca-4aec-8749-6777da45549e",
   "metadata": {},
   "source": [
    "# 24. SPO triples\n",
    "\n",
    "在知识图谱（Knowledge Graph，KG）中，SPO triples 是指知识图谱中的基本单元，表示事实或关系的三元组，包含以下三个部分：\n",
    "1. S（Subject）：\n",
    "\n",
    "主体，通常是一个实体，表示一个人、地点、物体、组织等。例如，“Albert Einstein”。\n",
    "\n",
    "2. P（Predicate）：\n",
    "\n",
    "谓词，表示主体与客体之间的关系或属性。例如，“was born in”。\n",
    "\n",
    "3. O（Object）：\n",
    "\n",
    "客体，通常是一个实体或属性值，表示与主体相关的对象。例如，“Ulm”（一个城市）。\n",
    "\n",
    "\n",
    "三元组的表示形式：\n",
    "\n",
    "`(S, P, O)`\n",
    "\n",
    "例如：\n",
    "\n",
    "$\\text{(Albert Einstein, was born in, Ulm)}$\n",
    "\n",
    "用途与意义：\n",
    "\n",
    "- 事实的表示：SPO 三元组用来表示真实世界中的某个事实。例如，上述三元组表示“爱因斯坦出生在乌尔姆”。\n",
    "- 关系的表达：通过谓词  P ，知识图谱可以捕获主体  S  与客体  O  之间的各种关系，例如所属关系、空间关系、时间关系等。\n",
    "- 图的构建：知识图谱实际上是一种图结构，节点是实体（主体和客体），边是它们之间的关系（谓词）。\n",
    "\n",
    "扩展：\n",
    "- Schema：谓词  P  通常会基于一个预定义的模式（schema），例如 RDF（Resource Description Framework）中的规范。\n",
    "- 属性与类型：如果  O  是一个值（如数字或日期），则表示属性；如果是另一个实体，则表示类型关系。\n",
    "\n",
    "实际应用：\n",
    "- 搜索引擎：例如，Google Knowledge Graph 中的关联搜索。\n",
    "- 问答系统：通过知识图谱提供基于事实的问答。\n",
    "- 推荐系统：基于实体关系推荐相关内容。\n",
    "\n",
    "示例扩展：\n",
    "\n",
    "知识图谱中的更多三元组可以描述更复杂的关系，例如：\n",
    "- $(Albert Einstein, won, Nobel Prize in Physics)$\n",
    "- $(Nobel Prize in Physics, awarded in, 1921)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c983d79-8039-4f55-9761-d43b1b32eca5",
   "metadata": {},
   "source": [
    "# 25. OpenIE (Open Information Extraction)\n",
    "OpenIE（Open Information Extraction，开放信息抽取）是一种自然语言处理（NLP）技术，用于从非结构化文本中自动提取结构化的语义信息，通常以三元组（Subject, Predicate, Object，简称 SPO）的形式表示。\n",
    "\n",
    "主要特点：\n",
    "1.\t开放性：\n",
    "\tOpenIE 不依赖于预定义的关系或领域知识，可以从任何领域的文本中抽取关系。\n",
    "\n",
    "\t例如，从句子“Albert Einstein was born in Ulm in 1879”中可以提取出：\n",
    "\n",
    "\t(Albert Einstein, was born in, Ulm)\n",
    "\n",
    "\t(Albert Einstein, was born in, 1879)\n",
    "\n",
    "2.\t自动化：\n",
    "\tOpenIE 工具可以自动识别文本中的实体和关系，不需要人为标注训练数据。\n",
    "\t\n",
    "3.\t领域无关性：\n",
    "\t适用于不同领域的文本，不局限于某一特定语料库。\n",
    "\n",
    "工作原理：\n",
    "\n",
    "OpenIE 的核心是从句子中识别事实性关系，包括以下步骤：\n",
    "1.\t文本分词与词性标注：\n",
    "\t对输入文本进行分词、词性标注，识别句子的组成部分。\n",
    "2.\t主语、谓语和宾语的识别：\n",
    "\t利用句法分析（如依存分析）提取主体、谓词和宾语。\n",
    "3.\t关系抽取：\n",
    "\t从文本中抽取候选的 SPO 三元组。\n",
    "4.\t过滤与优化：\n",
    "\t对抽取结果进行过滤，去除冗余信息，确保输出三元组的语义正确性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad638d-e22b-4088-af0d-ce5f681af5ef",
   "metadata": {},
   "source": [
    "# 26. Hybrid reasoning（混合推理）\n",
    "是一种结合多种推理方法的技术，用于处理复杂问题或知识表示。它在人工智能、知识图谱、语义网络和逻辑推理中被广泛应用。混合推理的核心思想是利用不同推理技术的优势，从而更全面、高效地解决问题。\n",
    "\n",
    "主要含义\n",
    "\n",
    "混合推理结合了两种或多种不同的推理方法，通常包括以下几种：\n",
    "1.\t符号推理（Symbolic Reasoning）：\n",
    "\n",
    "基于逻辑规则和明确的知识表示，例如一阶逻辑、规则推理。\n",
    "\n",
    "特点：精确性高，适合处理结构化和明确的知识。\n",
    "\n",
    "2.\t子符号推理（Sub-symbolic Reasoning）：\n",
    "\n",
    "基于数据驱动和概率模型，例如机器学习、深度学习。\n",
    "\n",
    "特点：适合处理非结构化数据（如文本、图像）和不确定性。\n",
    "\n",
    "3.\t基于知识的推理（Knowledge-based Reasoning）：\n",
    "\n",
    "使用知识库或知识图谱中的结构化知识。\n",
    "\n",
    "特点：依赖于领域知识，可以进行语义匹配和复杂查询。\n",
    "\n",
    "4.\t统计推理（Statistical Reasoning）：\n",
    "\n",
    "基于概率论和统计模型，处理不确定性。\n",
    "\n",
    "特点：适合处理海量数据和不确定性信息。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53868998-ebea-488e-aaba-5e19472fd98a",
   "metadata": {},
   "source": [
    "# 27. Multi-hop queries（多跳查询）\n",
    "是知识图谱、数据库和自然语言处理（NLP）领域的一种概念，指的是通过多个关系或路径跨越多个节点，从数据或知识图谱中检索相关信息的查询方式。\n",
    "\n",
    "在知识图谱或图数据库中，每个实体是一个节点，节点之间的关系用边连接。**多跳**是指在图中跨越多条边才能从起始节点到达目标节点。\n",
    "\n",
    "Single-hop query（单跳查询）：\n",
    "- 只需要经过一条边即可完成查询。\n",
    "- 例如：从“Albert Einstein”查找“出生地”（(Albert Einstein, born in, Ulm)）。\n",
    "\n",
    "Multi-hop query（多跳查询）：\n",
    "- 需要通过多个节点和边才能完成查询。\n",
    "- 例如：从“Albert Einstein”查找“所在国家”（跨过“出生地”找到对应的国家 (Albert Einstein, born in, Ulm) → (Ulm, located in, Germany)）。\n",
    "\n",
    "工作原理\n",
    "\n",
    "多跳查询通常依赖以下步骤：\n",
    "\n",
    "1.\t起点：从一个实体或节点开始。\n",
    "2.\t关系链：沿着预定义的关系链（路径）逐步扩展。\n",
    "3.\t目标节点：最终到达目标实体或满足查询条件的节点。\n",
    "\n",
    "在知识图谱中，多跳查询可以用路径描述，例如：\n",
    "\n",
    "$\\text{(Subject, Predicate1, Node1) → (Node1, Predicate2, Target)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67da9763-b8cf-435a-8668-402faa0916c2",
   "metadata": {},
   "source": [
    "`应用场景`\n",
    "1.\t知识图谱推理：查询复杂的语义关系。\n",
    "- 示例：\n",
    "\n",
    "问题：查找“爱因斯坦的研究领域”。\n",
    "\n",
    "路径：\n",
    "$(Albert Einstein, works on, Theory of Relativity) → (Theory of Relativity, belongs to, Physics)$\n",
    "\n",
    "2.\t问答系统（QA）：通过多跳推理回答复杂问题。\n",
    "- 示例：\n",
    "\n",
    "问题：爱因斯坦在哪个国家出生？\n",
    "\n",
    "多跳查询路径：\n",
    "$(Albert Einstein, born in, Ulm) → (Ulm, located in, Germany)$\n",
    "\n",
    "3.\t推荐系统：跨越用户兴趣和资源间的关系链，生成推荐。\n",
    "- 示例：\n",
    "\n",
    "用户 → 喜欢的电影 → 演员 → 演员出演的其他电影\n",
    "\n",
    "4.\t生物医学：查找基因、疾病和药物之间的复杂关系。\n",
    "- 示例：\n",
    "\n",
    "问题：某基因的相关疾病和药物是什么？\n",
    "\n",
    "路径：\n",
    "$(Gene, associated with, Disease) → (Disease, treated by, Drug)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e23c4-b498-4e6c-9654-92a1779c84dd",
   "metadata": {},
   "source": [
    "# 28. SPG (Semantic-enhanced Programmable Graph)\n",
    "[SPG](https://openspg.yuque.com/ndx6g9/wc9oyq/pkoaerwzmt0o37sk)\n",
    "\n",
    "L1: SPG Domain Constraint 领域模型约束\n",
    "\n",
    "实现知识的分类约束，构建事件/实体和概念常识/标准类型之间的归纳/传导/演绎等基础的领域语义能力\n",
    "\n",
    "L2: SPG Evolving 知识演化\n",
    "\n",
    "借助NLP技术呢你实现单类型内部各实例的链指/消歧/归一等\n",
    "\n",
    "L3: SPG Reasoning 知识推理\n",
    "\n",
    "借助语义谓词和逻辑规则构建可符号化表示的推理能力\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de925089-24e0-4e05-93a7-5bd7ce6e6728",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4331ef78-4a27-46b5-8fa4-fd95e12addfa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6e5d75-4d3c-4cfa-b40e-35940559820e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92f1d211-f1c1-4cf8-a508-4dcc9a800039",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "704c2817-d70e-49c8-8334-77a1ae1966b2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd96ccdd-ccce-4c8d-b2db-9a4765feca92",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cde06e-43bd-443d-b72a-e21ea906f52e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7646ec38-22d5-4deb-8270-bdfa717c98ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6f2473-96b5-4325-b3ea-a0a5fce68f60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a19501a-ac12-446d-b912-0313208245da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bea0014e-43a1-46b1-a941-7920168b4f49",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffc7ef19-fabc-48b0-9c97-12c7ef839839",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
