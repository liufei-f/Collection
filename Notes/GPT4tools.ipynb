{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cf59d1-b3cf-475a-81be-0361726c8d88",
   "metadata": {},
   "source": [
    "# GPT4tools: Teaching LLM to Use Tools via Self-instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d86ca-c252-425c-b3e0-8394ac76f43c",
   "metadata": {},
   "source": [
    "Utilize GPT-3.5 to generate tools-related instruction-following data, which is then used to tune the language model.\n",
    "This process offers language models the ability to access the multi-modal information by invoking visual models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b91419-43a3-44e2-8487-a1e62ea3c94f",
   "metadata": {},
   "source": [
    "## Dataset Construction\n",
    "1. Data Generation\n",
    "\n",
    "$ Y \\sim M_T(P_t|X_C) $\n",
    "\n",
    "$Y$: A large number of instruction-following data\n",
    "\n",
    "$X_C$: image content\n",
    "\n",
    "$M_T$: GPT-3.5\n",
    "\n",
    "$P_t$: A tool-related prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1267d3d7-a6b9-43df-893a-c775578b6e6c",
   "metadata": {},
   "source": [
    "The Pt comprises the system message, the definition of tools (`tool name : usage scenario, arguments`), and the suffix prompt which encourages $M_T$ to generate visual instructions and desired outputs. $Y$ , the outcome of $M_T$, consists of N instruction-output pairs ${y^1, y^2, ..., y^N }$, where $y^i$ has the format of \"`instruction, tool name, arguments`\", and N is the number of defined tools\n",
    "\n",
    "Without image context priors, GPT-3.5 tends to generate objects of visual instructions towards a small subset, which is reflected in t-SNE as sparser clusters.\n",
    "\n",
    "The language model tuned by image-conditioned data is more robust than models without the image content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7e19a-6200-4761-90ed-0d2e83995442",
   "metadata": {},
   "source": [
    "![jupyter](./figures/GPT4Tools_figure1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0721a0-4365-4ff3-a06c-f5ab8e20c398",
   "metadata": {},
   "source": [
    "2. Data Formation\n",
    "   \n",
    "   1. Remove duplicate instructions/incorrectly formatted instructions/calls with incorrect tool names/ calls with incorrect tool-arguments formats from the raw datasets (70K items -> 41K)\n",
    "\n",
    "   2. Transform the retained data into an instruction-response format utilizing a standardized template as shown in bottom-left corner of Fig.1. This procedure produces a new dataset, $Y^+_S$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf5024a-e9f4-49ea-a78b-cccf91b06879",
   "metadata": {},
   "source": [
    "$Y^+_S$ includes:\n",
    "\n",
    "`prefix prompt`: encompass system messages and tool definitions\n",
    "\n",
    "`image content`: the image content\n",
    "\n",
    "`user input`: replaced with the generated visual instruction\n",
    "\n",
    "Response:\n",
    "\n",
    "`Thought`: the model's cognition when to use tools\n",
    "\n",
    "`Action`: signifying which tools the model will use or action the model will take\n",
    "\n",
    "`Action input`: representing arguments of the selected tool\n",
    "\n",
    "`Observation`: reflecting outcomes of the used tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd7cbe4-fe4d-47eb-81a5-9fb63f6a2435",
   "metadata": {},
   "source": [
    "![jupyter](./figures/GPT4Tools_figure2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b90846f-676b-4029-acfc-75bc1c3ffd8a",
   "metadata": {},
   "source": [
    "3. Data Augmentation\n",
    "\n",
    "   Challenge: this simplistic format lacks complexity and depth in both instructions and responses.\n",
    "\n",
    "   Negative samples. The generated instructions primarily focus on tool usage, i.e., the decision after the Thought is always \"Yes\". Consequently, there is a potential risk that the fine-tuned model overfits such a decision. When the user instruction is not associated with the tool usage, the fine-tuned model may erroneously execute irrelevant actions by invoking unnecessary tools. To mitigate this issue, we synthesize negative samples YS− by selecting conversation data from the existing dataset [https://arxiv.org/abs/2304.03277] and converting them into the required template, as illustrated in Figure 3 (b). By tuning with $Y_S^+ ∪ Y_S^−$, the model can accurately decide when to use tools.\n",
    "\n",
    "    Context samples. The generated instructions adopt a standard and fixed `single-tune` format, which lacks a contextual structure. Thus, as shown in Figure 3 (c), we augment the dataset by cutting off the chain of action. We also randomly select multiple instructions from $Y_S^+ ∪ Y_S^−$ and reformat them into multi-turn conversation data. In this way, we synthesize the contextual instruction-following data $Y_S^c$, enabling the tuned model to call tools within the given context.\n",
    "\n",
    "\n",
    "Total dataset:$$Y_S = Y_S^+ ∪ Y_S^− ∪ Y_S^c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80010ce3-2b41-4ae9-bdda-6700bfeb8f2d",
   "metadata": {},
   "source": [
    "## Instruction Tuning\n",
    "\n",
    "tune the off-the-self language model using its original auto-regressive training objective\n",
    "\n",
    "leverage LoRA optimization, which freezes the language model and only optimizes rank decomposition components of the Transformer layers\n",
    "\n",
    "For a sequence with $L$ tokens, compute the probability of the target response $X_r$ by:\n",
    "$$\n",
    "p(X_r \\mid X_C, X_{\\text{inst}}) = \\prod_{i=1}^{L} p_\\theta(x_i \\mid X_C, X_{\\text{inst}}, x_{1:i-1}),\n",
    "$$\n",
    "\n",
    "$X_{\\text{inst}}$: instruction tokens\n",
    "\n",
    "$\\theta$: trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c514ef-9bf7-4055-b29b-9f24b834fbc1",
   "metadata": {},
   "source": [
    "## Evaluation Approach\n",
    "construct an evaluation dataset following the same procedures detailed in § 3.1 and manually verify the accuracy of each item\n",
    "\n",
    "Evaluation dateset:\n",
    "1. validation set: has the same ingredients as the training set, encompassing 23 tools (defined in Visual ChatGPT)\n",
    "   \n",
    "   validate whether the model can adhere to user commands correctly after tuning with the training set.\n",
    "3. test set: comprises 8 novel tools absent from the training set\n",
    "   \n",
    "   verify whether the model can generalize to new tools after tuning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7671d-23a7-45de-8c8c-e3eff51dc6ec",
   "metadata": {},
   "source": [
    "Based on the human-annotated evaluation dataset with N instructions, a successful rate to measure the model’s performance from three aspects is designed:\n",
    "\n",
    "\n",
    "- **Successful Rate of Thought**($SR_t$) measures whether the predicted decision matches the ground-truth decision. It is calculated as $$SR_t = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(\\tau_i),$$ where $\\tau_i$ signifies a singular process. If the thought is correct, $\\mathbb{I}(\\tau_i)$ is equal to 1, and 0 otherwise.\n",
    "\n",
    "- **Successful Rate of Action** ($SR_{act}$) measures whether the predicted tool name is in agreement with the name of the ground truth tool. It is calculated as $$SR_{act} = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(\\alpha_i),$$ where $\\alpha_i$ denotes the matching process for the tool names. In cases where the predicted tool name matches the pre-defined name, $\\mathbb{I}(\\alpha_i)$ is equal to 1, and 0 otherwise.\n",
    "\n",
    "- **Successful Rate of Arguments** ($SR_{args}$) evaluates whether the predicted arguments match the ground-truth arguments. It can be calculated using the following equation:\n",
    "$$SR_{\\text{args}} = \\frac{1}{N} \\sum_{i=1}^N \\eta_i, \\quad \\text{where } \\eta_i = \\frac{1}{K} \\sum_{j} \\eta_{i,j}.$$\n",
    "\n",
    "$\\eta_i$ denotes a sequence of arguments encompassing both the image path and the input text. \n",
    "\n",
    "$K$ represents the quantity of arguments in $\\eta_i$. When the argument belongs to the image path, $\\eta_{i,j}$ equals 1 if the predicted and ground-truth image paths share the same suffix, and 0 otherwise. When the argument is the input text, $\\eta_{i,j}$ is equal to the BLEU score between the predicted and the ground truth text.\n",
    "- **Successful Rate** ($SR$) measures whether a chain of actions are executed successfully, which requires the correctness of thought, tool name, and tool arguments at the same time:\n",
    "$$SR = \\frac{1}{N} \\sum_{i=1}^N \\mathbb{I}(\\tau_i) \\cdot \\mathbb{I}(\\alpha_i) \\cdot \\mathbb{I}(\\eta_i>0.5),$$\n",
    "Additionally, when a procedure comprises two consecutive actions, the SR equals 100% only if\n",
    "both actions are executed correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd6015d-3e76-4ddc-86af-34ce7d01fa21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14320f0b-7211-41ff-b6ac-bb26bca0332c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "283cb3df-2560-4793-ade3-130e82bb1468",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
