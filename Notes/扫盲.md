

- [1. Zero-shot learning(零样本学习)](#1-zero-shot-learning零样本学习)
- [2. Logical Reasoning(逻辑推理)](#2-logical-reasoning逻辑推理)
- [3. Low-Rank Adaptation(LoRA)](#3-low-rank-adaptationlora)
  - [3.1 Low-Rank Matrix](#31-low-rank-matrix)
- [4. Multimodal Model(多模态模型)](#4-multimodal-model多模态模型)



# 1. Zero-shot learning(零样本学习)
模型能够在没有见过特定任务或样本标签的情况下，对新任务进行预测或分类。
它依赖于模型对任务和数据的通用理解，通常通过以下方式实现：
1. 利用先验知识：预训练
2. 任务描述：通过自然语言描述任务（如文本提示）来指导模型预测，而无需提供特定的训练样本
3. 共享特征空间：将未知类别的特征映射到已知类别共享的特征空间，基于语义或上下文进行推断

# 2. Logical Reasoning(逻辑推理)
模型根据已知事实/规则或模型进行逻辑分析和推导的能力。
1. Deductive reasoning: 从一般规则推导出具体结论
2. Inductive reasoning: 从具体例子总结出一般规律
3. Abductive reasoning: 从结论推测最可能的原因

# 3. Low-Rank Adaptation(LoRA)
是一种高效适配预训练模型的方法，尤其在大规模语言模型或其他深度学习模型上表现出色。它通过引入低秩分解的思想，大幅减少模型参数调整的需求，从而降低计算开销和存储需求。

在深度学习中，预训练的大模型通常含有大量的参数，直接对其进行fine-tuning需要巨大的计算资源和存储空间。LoRA通过引入低秩矩阵，将权重调整在一个低秩子空间中，极大的减少了参数调整的数量。
1. 权重矩阵分解
假设一个预训练模型的权重矩阵为$W$，LoRA假设权重的调$\Delta W$是低秩的，可以分解为两个更小的矩阵：$\Delta W = A \cdot B$
其中：
  $ A \in \mathbb{R}^{d \times r} $
  $ B \in \mathbb{R}^{r \times k} $
  $ r  是分解的秩（通常远小于d和k）$
2. 冻结原始权重
在训练过程中，原始权重$W$保持冻结状态，仅更新低秩矩阵$A$和$B$。这不仅减少了需要训练的参数，还保留了预训练模型的知识。
3. 计算效率
由于低秩矩阵的计算复杂度比完整权重矩阵低得多，LoRA显著提升了适配速度，同时降低了显存占用。

## 3.1 Low-Rank Matrix
在线性代数中，低秩矩阵是指一个矩阵的秩（rank）比它的行数或列数要小得多。简单来说，它是一种结构化的矩阵，表示其数据冗余性较高，很多信息可以用更少的自由度来表示。

矩阵的秩（Rank）: 矩阵的秩是线性无关行或列的数量。
- 对于一个 $ m \times n $ 的矩阵  A ：
  全秩矩阵：如果 $ \text{rank}(A) = \min(m, n) $，矩阵信息完整，行或列是线性无关的。
  低秩矩阵：如果 $ \text{rank}(A) \ll \min(m, n) $，矩阵存在冗余，行或列是高度相关的。

低秩矩阵的实际例子

1. 图像处理：
在图像压缩中，图片的像素矩阵通常是近似低秩的，可以通过降维技术（如PCA）压缩数据。
2. 推荐系统：
用户-物品评分矩阵通常是稀疏的（大部分项为空），也可以近似为低秩矩阵，因为用户的评分行为通常可以归因于少数几个潜在特征（如偏好类型、价格敏感性等）。
3. 自然语言处理：
在语言模型中，大型权重矩阵可以近似为低秩形式，用来减少参数量并提升计算效率（例如LoRA方法）。
低秩分解（Low-Rank Decomposition）

为了利用低秩矩阵的特性，我们可以将它分解成多个更小的矩阵：

$ A \approx U \cdot V^T $

$A$ ：原始矩阵
$U$ ：表示特征的矩阵
$V^T$ ：表示组合权重的矩阵
秩  $r$ ：通常 $ r \ll \min(m, n) $

例如，通过奇异值分解（SVD）：

$ A = U \Sigma V^T $

我们可以用较小的奇异值（低秩近似）来近似原始矩阵。

# 4. Multimodal Model(多模态模型)

多模态模型 (Multimodal Model) 是指能够同时处理和整合多种不同类型（模态）的数据的人工智能模型。模态是数据的类型或形式，例如：
- 视觉模态：图像、视频
- 语言模态：文本、语音
- 感觉模态：触觉、气味
- 其他模态：传感器数据、生物信号（如脑电波或心电图）

通过整合多个模态的数据，模型可以更全面地理解和分析复杂的任务或场景。

多模态模型的架构设计

1.	单模态特征提取：
•	每种模态使用专门的网络进行特征提取。例如：
•	图像：卷积神经网络（CNN）
•	文本：循环神经网络（RNN）或变换器（Transformer）
2.	模态间融合：
•	提取的特征通过以下方式进行融合：
•	早期融合：在特征提取之前就合并不同模态的原始数据。
•	中期融合：单独提取特征后，将这些特征在某一层合并。
•	后期融合：各模态单独完成部分任务，再将结果整合。
3.	联合优化：
•	模型在训练时，目标函数能够优化所有模态之间的联合表现。

经典多模态模型示例

1.	CLIP (Contrastive Language–Image Pretraining)：
•	由 OpenAI 开发，可以同时理解图像和文字，通过对比学习捕捉二者的语义关联。
•	应用于图像检索和跨模态理解。
2.	DALL·E：
•	基于文本生成图像的模型，将文字和视觉模态紧密结合。
3.	VideoBERT：
•	将语言模型扩展到视频分析，通过视频帧和字幕数据联合建模。

